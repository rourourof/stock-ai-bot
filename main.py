import os
import requests
import datetime
import pytz
import yfinance as yf
import feedparser
import time
from newsapi import NewsApiClient
from discord_webhook import DiscordWebhook

# ==========================================
# 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ3ã¤ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ï¼‰
# ==========================================

def fetch_all_news():
    jst = pytz.timezone('Asia/Tokyo')
    # ç›´è¿‘2æ—¥é–“ã®æƒ…å ±ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹
    start_date = (datetime.datetime.now(jst) - datetime.timedelta(days=2)).strftime('%Y-%m-%d')
    
    news_content = ""

    # --- ã‚½ãƒ¼ã‚¹A: NewsAPI (ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢) ---
    try:
        api_key = os.getenv("NEWS_API_KEY")
        if api_key:
            newsapi = NewsApiClient(api_key=api_key)
            # NVIDIAãŠã‚ˆã³ç±³å›½å¸‚å ´ã«é–¢ã™ã‚‹æœ€æ–°è¨˜äº‹ã‚’å–å¾—
            res = newsapi.get_everything(q="NVIDIA OR 'US Stock Market'", language='en', from_param=start_date, sort_by='publishedAt', page_size=8)
            news_content += "\nã€NewsAPI: ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢é€Ÿå ±ã€‘\n"
            for art in res.get('articles', []):
                news_content += f"- {art['publishedAt']} | {art['source']['name']}: {art['title']}\n"
    except Exception as e:
        news_content += f"\nã€NewsAPIã€‘å–å¾—ã‚¨ãƒ©ãƒ¼: {e}\n"

    # --- ã‚½ãƒ¼ã‚¹B: Alpha Vantage (é‡‘èã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ) ---
    try:
        av_key = os.getenv("AV_API_KEY")
        if av_key:
            url = f"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=NVDA&apikey={av_key}"
            r = requests.get(url, timeout=15)
            data = r.json()
            news_content += "\nã€AlphaVantage: æŠ•è³‡å®¶å¿ƒç†/ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã€‘\n"
            # ä¸Šä½5ä»¶ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆä»˜ããƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
            for item in data.get('feed', [])[:5]:
                sentiment = item.get('overall_sentiment_label', 'ä¸æ˜')
                news_content += f"- {item['title']} (å¸‚å ´å¿ƒç†: {sentiment})\n"
    except Exception as e:
        news_content += f"\nã€AlphaVantageã€‘å–å¾—ã‚¨ãƒ©ãƒ¼: {e}\n"

    # --- ã‚½ãƒ¼ã‚¹C: Google News (RSS/è¶…é€Ÿå ±) ---
    try:
        news_content += "\nã€Google News: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œç´¢çµæœã€‘\n"
        # è‹±èªåœã®æœ€æ–°æƒ…å ±ã‚’å–å¾—
        feed = feedparser.parse("https://news.google.com/rss/search?q=NVIDIA+stock+2026&hl=en-US&gl=US&ceid=US:en")
        for entry in feed.entries[:6]:
            news_content += f"- {entry.published} | {entry.title}\n"
    except Exception as e:
        news_content += f"\nã€Google Newsã€‘å–å¾—ã‚¨ãƒ©ãƒ¼: {e}\n"

    return news_content

# ==========================================
# 2. å¸‚å ´æ•°å€¤ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚»ã‚¯ã‚·ãƒ§ãƒ³
# ==========================================

def get_market_summary():
    try:
        # NVIDIAã¨ä¸»è¦æŒ‡æ•°ã®ç¾åœ¨å€¤ã‚’å–å¾—
        tickers = {"NVDA": "NVIDIA", "^SOX": "åŠå°ä½“æŒ‡æ•°", "ES=F": "S&P500å…ˆç‰©"}
        summary = "\nã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¸‚å ´æ•°å€¤ã€‘\n"
        for sym, name in tickers.items():
            t = yf.Ticker(sym)
            h = t.history(period="5d")
            if len(h) < 2: continue
            curr = h.iloc[-1]['Close']
            prev = h.iloc[-2]['Close']
            diff = ((curr - prev) / prev) * 100
            summary += f"- {name}: {curr:.2f} ({diff:+.2f}%)\n"
        return summary
    except:
        return "\nã€å¸‚å ´æ•°å€¤ã€‘å–å¾—å¤±æ•—\n"

# ==========================================
# 3. AIï¼ˆGeminiï¼‰ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ & é€ä¿¡
# ==========================================

def main():
    # æ—¥æœ¬æ™‚é–“ã®å–å¾—
    jst = pytz.timezone('Asia/Tokyo')
    now = datetime.datetime.now(jst)
    date_str = now.strftime('%Y/%m/%d %H:%M')

    # å„ç¨®ãƒ‡ãƒ¼ã‚¿ã®åé›†
    factual_news = fetch_all_news()
    market_data = get_market_summary()

    # AIã¸ã®æŒ‡ç¤ºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
    # temperature=0.0 ã«ã‚ˆã‚Šã€æä¾›ãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®ã€Œå˜˜ã€ã‚’å®Œå…¨ã«å°ã˜ã‚‹
    prompt = f"""
ã‚ãªãŸã¯æ©Ÿé–¢æŠ•è³‡å®¶å‘ã‘ã®ã€äº‹å®Ÿç¢ºèªå°‚é–€ã€‘ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆã§ã™ã€‚
æœ¬æ—¥: {date_str}

ã€å³å®ˆï¼šãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå‰µä½œãƒ»ä»®èª¬ï¼‰ã®ç¦æ­¢ã€‘
1. ä¸‹è¨˜ã®ã€Œå®Ÿåœ¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã€ãŠã‚ˆã³ã€Œå¸‚å ´æ•°å€¤ã€ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„æƒ…å ±ã¯ã€çµ¶å¯¾ã«ãƒ¬ãƒãƒ¼ãƒˆã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚
2. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€ç„¡ç†ã«å†…å®¹ã‚’ä½œã‚‰ãšã€ã€Œç‰¹ç­†ã™ã¹ãæ–°è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—ã€ã¨äº‹å®Ÿã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚
3. 2024å¹´ã‚„2025å¹´ã®å‡ºæ¥äº‹ã‚’ç¾åœ¨ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨ã—ã¦æ‰±ã‚ãªã„ã§ãã ã•ã„ã€‚

ã€æä¾›ã•ã‚ŒãŸå®Ÿåœ¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã€‘
{factual_news}

ã€æä¾›ã•ã‚ŒãŸå¸‚å ´æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã€‘
{market_data}

æ§‹æˆï¼ˆå†·é™ã€å®¢è¦³çš„ãªãƒˆãƒ¼ãƒ³ã§åŸ·ç­†ã›ã‚ˆï¼‰:
1. ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¯ãƒˆè¦ç´„ï¼ˆã©ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®æƒ…å ±ã‹æ˜è¨˜ï¼‰
2. å¸‚å ´æ•°å€¤ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«åˆ†æï¼ˆä¹–é›¢ç‡ã‚„é¨°è½ï¼‰
3. äº‹å®Ÿã«åŸºã¥ãä»Šå¤œã®ãƒãƒ¼ã‚±ãƒƒãƒˆè¦‹é€šã—ï¼ˆè«–ç†çš„ãªã‚·ãƒŠãƒªã‚ªï¼‰
"""

    # Gemini APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆOpenRouterçµŒç”±ï¼‰
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "google/gemini-2.0-flash-exp:free",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.0
    }

    try:
        response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload, timeout=120)
        report = response.json()['choices'][0]['message']['content']

        # Discordã¸é€ä¿¡
        webhook_url = os.getenv("DISCORD_WEBHOOK_URL")
        if webhook_url:
            # é•·æ–‡å¯¾ç­–ã§åˆ†å‰²é€ä¿¡
            chunks = [report[i:i+1800] for i in range(0, len(report), 1800)]
            for i, chunk in enumerate(chunks):
                title = f"ğŸ“‘ **Institutional Daily Report ({date_str}) P{i+1}**\n" if i == 0 else ""
                DiscordWebhook(url=webhook_url, content=title + chunk).execute()
                time.sleep(1)
        else:
            print(report)
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()
